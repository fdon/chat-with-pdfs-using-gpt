{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ba6108-588a-470b-8ea3-86521985ec4c",
   "metadata": {},
   "source": [
    "# Chat with PDFs using ChatGPT & OpenAI GPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f1d5ed-de6d-4edb-8c02-5abe8631c5bd",
   "metadata": {},
   "source": [
    "This is a supplementary python notebook for the blog - https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-gpt-api/. We dive into a detailed code tutorial on how to chat with all kinds of PDF files using OpenAI GPT API and use it for PDF automations / chatbots.\n",
    "\n",
    "* We will chat with PDFs using just a few lines of Python code.\n",
    "* We will chat with large PDF files using ChatGPT API and LangChain.\n",
    "* We will build an automation to sort PDF files based on their contents.\n",
    "* We will go through examples of building more automations for tasks involving PDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8b5d6-dc01-4ea3-a4aa-0b9065894da3",
   "metadata": {},
   "source": [
    "## Chat with PDF using ChatGPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49317bea-fcb7-4473-bf1c-1603ca18d67b",
   "metadata": {},
   "source": [
    "Let us now chat with our first PDF using OpenAI's GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ef09a-3a36-4e14-8477-6204a11e3495",
   "metadata": {},
   "source": [
    "We are going to converse with a resume PDF to demonstrate this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057ed90-7e39-41e0-b2ef-fa92dba71b1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 1 - Read the PDF File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2820fd2c-62e2-4f9a-a45b-151222a7cc2b",
   "metadata": {},
   "source": [
    "We follow different approaches based on whether the PDF is scanned or digital."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01cea90-d384-4a2b-a7fa-8f00ccc21b1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Approach 1 : Read Digital PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b1606fd-0b0b-4c80-a99d-a4fe5b68385a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /home/fd/mambaforge/lib/python3.10/site-packages (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd36aa92-9c07-4c79-b107-0ac6b90b6e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUNCTIONAL  (EXPERIENCED)   \n",
      "IM A . SAMPLE I  \n",
      "1234 North 55 Street  \n",
      "Bellevue, Nebraska 68005  \n",
      "(402) 292 -2345  \n",
      "imasample1@xxx.com  \n",
      " \n",
      "SUMMARY OF QUALIFICATIONS  \n",
      "Exceptionally well organized and resourceful Professional  with more than six years experience and a \n",
      "solid academic background in accounting and financial management; excellent analytical and problem \n",
      "solving skills; able to handle multiple projects while pr oducing high quality work in a fast -paced, \n",
      "deadline -oriented environment.  \n",
      " \n",
      "EDUCATION  \n",
      "Bachelor of Science , Bellevue University, Bellevue, NE (In Progress)  \n",
      " Major:  Accounting  Minor:  Computer Information Systems  \n",
      " Expected Graduation Date:  January, 20xx  GPA  to date:  3.95/4.00  \n",
      " \n",
      "PROFESSIONAL ACCOMPLISHMENTS  \n",
      "Accounting and Financial Management  \n",
      " Developed and maintained accounting records for up to fifty bank accounts.  \n",
      " Formulated monthly and year -end financial statements and generated various payroll records, \n",
      "including federal and state payroll reports, annual tax reports, W -2 and 1099 forms, etc.  \n",
      " Tested accuracy of account balances and prepared supporting documentation for submission during a \n",
      "comprehensive three -year audit of financial operations.  \n",
      " Formulated int ricate pro -forma budgets.  \n",
      " Calculated and implemented depreciation/amortization schedules.  \n",
      "Information Systems Analysis and Problem Solving  \n",
      " Converted manual to computerized accounting systems for two organizations.  \n",
      " Analyzed and successfully reprogrammed sof tware to meet customer requirements.  \n",
      " Researched and corrected problems to assure effective operation of newly computerized systems.  \n",
      " \n",
      "WORK HISTORY  \n",
      "Student Intern , Financial Accounting Development Program, Mutual of Omaha, Omaha, NE  \n",
      "(Summer 20xx)  \n",
      "Accounting Coordinator , Nebraska Special Olympics, Omaha, NE (20xx -20xx)  \n",
      "Bookkeeper , SMC, Inc., Omaha, NE (20xx – 20xx)  \n",
      "Bookkeeper , First United Methodist Church, Altus, OK (20xx – 20xx)  \n",
      " \n",
      "PROFESSIONAL AFFILIATION  \n",
      "Member,  IMA, Bellevue University Student Chapter  \n",
      " \n",
      "COMP UTER SKILLS  \n",
      " Proficient in MS Office (Word, Excel, PowerPoint, Outlook), QuickBooks  \n",
      " Basic Knowledge of MS Access, SQL, Visual Basic, C++  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "pdf_file_obj = open('resume-sample.pdf', 'rb')\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file_obj)\n",
    "num_pages = len(pdf_reader.pages)\n",
    "detected_text = ''\n",
    "\n",
    "for page_num in range(num_pages):\n",
    "    page_obj = pdf_reader.pages[page_num]\n",
    "    detected_text += page_obj.extract_text() + '\\n\\n'\n",
    "\n",
    "pdf_file_obj.close()\n",
    "\n",
    "print(detected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058399a7-019a-4cbd-a276-a45b6d7a861d",
   "metadata": {},
   "source": [
    "#### Step 2 - First Chat with PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d602b7b-1335-49fe-a13a-2243e4af3a89",
   "metadata": {},
   "source": [
    "Let us ask the LLM to suggest jobs that this person will be suitable for based on his resume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57a3256-3bbf-4653-927e-88e33b8fb56b",
   "metadata": {},
   "source": [
    "Firstly, We import the os and openai library and define our OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e74c9f1-0fbb-4ffa-a296-44da367e6d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = 'mykey'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04936e0-574b-42c6-a0b9-d8a8f83368d9",
   "metadata": {},
   "source": [
    "Choosing the ideal model while using OpenAI's python library depends on your use case and specific requirements. We recommend going through the list of available models and learning the pros and cons of each of the available models. You can access the list of available models as follows - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb9dc66-c557-4516-aa71-75bb3697c15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>object</th>\n",
       "      <th>created</th>\n",
       "      <th>owned_by</th>\n",
       "      <th>permission</th>\n",
       "      <th>root</th>\n",
       "      <th>parent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text-davinci-001</td>\n",
       "      <td>model</td>\n",
       "      <td>1649364042</td>\n",
       "      <td>openai</td>\n",
       "      <td>[{'id': 'modelperm-CDlahk1RbkghXDjtxqzXoPNo', ...</td>\n",
       "      <td>text-davinci-001</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text-search-curie-query-001</td>\n",
       "      <td>model</td>\n",
       "      <td>1651172509</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>[{'id': 'modelperm-fNgpMH6ZEQulSq1CjzlfQuIe', ...</td>\n",
       "      <td>text-search-curie-query-001</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>davinci</td>\n",
       "      <td>model</td>\n",
       "      <td>1649359874</td>\n",
       "      <td>openai</td>\n",
       "      <td>[{'id': 'modelperm-8s5tCuiXSr3zT00nLwZGyMpS', ...</td>\n",
       "      <td>davinci</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text-babbage-001</td>\n",
       "      <td>model</td>\n",
       "      <td>1649364043</td>\n",
       "      <td>openai</td>\n",
       "      <td>[{'id': 'modelperm-YABzYWjC1kS6M2BnI6Fr9vuS', ...</td>\n",
       "      <td>text-babbage-001</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>curie-instruct-beta</td>\n",
       "      <td>model</td>\n",
       "      <td>1649364042</td>\n",
       "      <td>openai</td>\n",
       "      <td>[{'id': 'modelperm-4GYfzAdSMcJmQvF7bsw01UWw', ...</td>\n",
       "      <td>curie-instruct-beta</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>model</td>\n",
       "      <td>1669599635</td>\n",
       "      <td>openai-internal</td>\n",
       "      <td>[{'id': 'modelperm-a6niqBmW2JaGmo0fDO7FEt1n', ...</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>davinci-similarity</td>\n",
       "      <td>model</td>\n",
       "      <td>1651172509</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>[{'id': 'modelperm-XHJ9P2cvfDAl6Q6NABs6wD7G', ...</td>\n",
       "      <td>davinci-similarity</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>code-davinci-edit-001</td>\n",
       "      <td>model</td>\n",
       "      <td>1649880484</td>\n",
       "      <td>openai</td>\n",
       "      <td>[{'id': 'modelperm-T8Ie7SvlPyvtsDvPlfC8DftZ', ...</td>\n",
       "      <td>code-davinci-edit-001</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>text-similarity-curie-001</td>\n",
       "      <td>model</td>\n",
       "      <td>1651172507</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>[{'id': 'modelperm-ZQZGhVQCQSN4WC1wRJsFZfRL', ...</td>\n",
       "      <td>text-similarity-curie-001</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>text-embedding-ada-002</td>\n",
       "      <td>model</td>\n",
       "      <td>1671217299</td>\n",
       "      <td>openai-internal</td>\n",
       "      <td>[{'id': 'modelperm-F3BGCNGb0ChzFesHIYjbNYUX', ...</td>\n",
       "      <td>text-embedding-ada-002</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id object     created         owned_by  \\\n",
       "0             text-davinci-001  model  1649364042           openai   \n",
       "1  text-search-curie-query-001  model  1651172509       openai-dev   \n",
       "2                      davinci  model  1649359874           openai   \n",
       "3             text-babbage-001  model  1649364043           openai   \n",
       "4          curie-instruct-beta  model  1649364042           openai   \n",
       "5             text-davinci-003  model  1669599635  openai-internal   \n",
       "6           davinci-similarity  model  1651172509       openai-dev   \n",
       "7        code-davinci-edit-001  model  1649880484           openai   \n",
       "8    text-similarity-curie-001  model  1651172507       openai-dev   \n",
       "9       text-embedding-ada-002  model  1671217299  openai-internal   \n",
       "\n",
       "                                          permission  \\\n",
       "0  [{'id': 'modelperm-CDlahk1RbkghXDjtxqzXoPNo', ...   \n",
       "1  [{'id': 'modelperm-fNgpMH6ZEQulSq1CjzlfQuIe', ...   \n",
       "2  [{'id': 'modelperm-8s5tCuiXSr3zT00nLwZGyMpS', ...   \n",
       "3  [{'id': 'modelperm-YABzYWjC1kS6M2BnI6Fr9vuS', ...   \n",
       "4  [{'id': 'modelperm-4GYfzAdSMcJmQvF7bsw01UWw', ...   \n",
       "5  [{'id': 'modelperm-a6niqBmW2JaGmo0fDO7FEt1n', ...   \n",
       "6  [{'id': 'modelperm-XHJ9P2cvfDAl6Q6NABs6wD7G', ...   \n",
       "7  [{'id': 'modelperm-T8Ie7SvlPyvtsDvPlfC8DftZ', ...   \n",
       "8  [{'id': 'modelperm-ZQZGhVQCQSN4WC1wRJsFZfRL', ...   \n",
       "9  [{'id': 'modelperm-F3BGCNGb0ChzFesHIYjbNYUX', ...   \n",
       "\n",
       "                          root parent  \n",
       "0             text-davinci-001   None  \n",
       "1  text-search-curie-query-001   None  \n",
       "2                      davinci   None  \n",
       "3             text-babbage-001   None  \n",
       "4          curie-instruct-beta   None  \n",
       "5             text-davinci-003   None  \n",
       "6           davinci-similarity   None  \n",
       "7        code-davinci-edit-001   None  \n",
       "8    text-similarity-curie-001   None  \n",
       "9       text-embedding-ada-002   None  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "models = openai.Model.list()\n",
    "modelsdf = pd.DataFrame(models[\"data\"])\n",
    "modelsdf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ab193-5ecd-48d4-a4b6-d4353262a80c",
   "metadata": {},
   "source": [
    "Next, we append our query - \"give a list of jobs suitable for the above resume\" to the extracted PDF text and send this as the user_msg. The detected_text variable already contains the data extracted from the PDF. We will simply append our query here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d225142-7b7d-4ca2-9acb-b7fd32eb22c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'give a list of jobs suitable for the above resume.'\n",
    "\n",
    "user_msg = detected_text + '\\n\\n' + query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be3b7e7-20a3-4930-bb66-fa6c9fdd6dc1",
   "metadata": {},
   "source": [
    "We also add a relevant system_msg to refine the behavior of the AI assistant. In our case, a useful system message can be \"You are a helpful career advisor.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10d5ca7a-9d31-4eb4-88ef-ac836fa3a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = 'You are a helpful career advisor.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc733f-b44c-46f3-a975-c6462b116c3a",
   "metadata": {},
   "source": [
    "We send the request to get our first response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e5c085f-851d-439e-8c37-27f0d6e9929d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUNCTIONAL  (EXPERIENCED)   \n",
      "IM A . SAMPLE I  \n",
      "1234 North 55 Street  \n",
      "Bellevue, Nebraska 68005  \n",
      "(402) 292 -2345  \n",
      "imasample1@xxx.com  \n",
      " \n",
      "SUMMARY OF QUALIFICATIONS  \n",
      "Exceptionally well organized and resourceful Professional  with more than six years experience and a \n",
      "solid academic background in accounting and financial management; excellent analytical and problem \n",
      "solving skills; able to handle multiple projects while pr oducing high quality work in a fast -paced, \n",
      "deadline -oriented environment.  \n",
      " \n",
      "EDUCATION  \n",
      "Bachelor of Science , Bellevue University, Bellevue, NE (In Progress)  \n",
      " Major:  Accounting  Minor:  Computer Information Systems  \n",
      " Expected Graduation Date:  January, 20xx  GPA  to date:  3.95/4.00  \n",
      " \n",
      "PROFESSIONAL ACCOMPLISHMENTS  \n",
      "Accounting and Financial Management  \n",
      " Developed and maintained accounting records for up to fifty bank accounts.  \n",
      " Formulated monthly and year -end financial statements and generated various payroll records, \n",
      "including federal and state payroll reports, annual tax reports, W -2 and 1099 forms, etc.  \n",
      " Tested accuracy of account balances and prepared supporting documentation for submission during a \n",
      "comprehensive three -year audit of financial operations.  \n",
      " Formulated int ricate pro -forma budgets.  \n",
      " Calculated and implemented depreciation/amortization schedules.  \n",
      "Information Systems Analysis and Problem Solving  \n",
      " Converted manual to computerized accounting systems for two organizations.  \n",
      " Analyzed and successfully reprogrammed sof tware to meet customer requirements.  \n",
      " Researched and corrected problems to assure effective operation of newly computerized systems.  \n",
      " \n",
      "WORK HISTORY  \n",
      "Student Intern , Financial Accounting Development Program, Mutual of Omaha, Omaha, NE  \n",
      "(Summer 20xx)  \n",
      "Accounting Coordinator , Nebraska Special Olympics, Omaha, NE (20xx -20xx)  \n",
      "Bookkeeper , SMC, Inc., Omaha, NE (20xx – 20xx)  \n",
      "Bookkeeper , First United Methodist Church, Altus, OK (20xx – 20xx)  \n",
      " \n",
      "PROFESSIONAL AFFILIATION  \n",
      "Member,  IMA, Bellevue University Student Chapter  \n",
      " \n",
      "COMP UTER SKILLS  \n",
      " Proficient in MS Office (Word, Excel, PowerPoint, Outlook), QuickBooks  \n",
      " Basic Knowledge of MS Access, SQL, Visual Basic, C++  \n",
      "\n",
      "\n",
      "\n",
      "give a list of jobs suitable for the above resume.\n"
     ]
    }
   ],
   "source": [
    "print(user_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e63bb745-1e83-4e0b-a59e-03b9e08e004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                    {\"role\": \"user\", \"content\": user_msg}]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b908b03-b3f8-4493-8c7b-5ed9b4b12048",
   "metadata": {},
   "source": [
    "Once the request is complete, the response object will contain the response from the LLM. We can view it by accessing the 'choices' attribute in the response object as follows -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c65372e2-17e4-4f25-ad14-c7225f89e0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the resume, here is a list of job positions that could be suitable:\n",
      "\n",
      "1. Accountant\n",
      "2. Financial Analyst\n",
      "3. Bookkeeper\n",
      "4. Budget Analyst\n",
      "5. Financial Reporting Specialist\n",
      "6. Audit Associate/Assistant\n",
      "7. Financial Systems Analyst\n",
      "8. Payroll Specialist\n",
      "9. Inventory Accountant\n",
      "10. Tax Accountant\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c13aec-3949-4cc3-a1cc-423247d2b1df",
   "metadata": {},
   "source": [
    "#### Step 3 : Continuing the Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d9a9a3-9fea-4d39-b71c-5697ef868863",
   "metadata": {},
   "source": [
    "Often, we would want to have conversations with the LLM which are more than just a pair of a single prompt and a single response. Let us now learn how to use our past conversation history to continue the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4239ad3-27b1-4a47-a092-005132d5e991",
   "metadata": {},
   "source": [
    "To simplify the implementation, we define the following function for calling the OpenAI GPT API from now on -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee6681b7-d81d-4f23-9508-0852ae81ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_chat(system_message, user_assistant_messages):\n",
    "  \n",
    "  system_msg = [{\"role\": \"system\", \"content\": system_message}]\n",
    "  \n",
    "  user_assistant_msgs = [{\"role\": \"assistant\", \"content\": user_assistant_messages[i]} if i % 2 else {\"role\": \"user\", \"content\": user_assistant_messages[i]} for i in range(len(user_assistant_messages))]\n",
    "\n",
    "  allmsgs = system_msg + user_assistant_msgs\n",
    "  response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                          messages=allmsgs)\n",
    "  \n",
    "  return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb502de-47c6-48d4-9768-f85d202a8619",
   "metadata": {},
   "source": [
    "The function accepts -\n",
    "\n",
    "* system_message (string) : This acts as the system_msg\n",
    "* user_assistant_messages (list) : This list contains user prompts and model responses in alternating order. This is also the order in which they occur in the conversation.\n",
    "\n",
    "The function internally makes the API call to generate and return a new response based on the conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c10e06-53f9-4f3a-a903-abc1252e07f8",
   "metadata": {},
   "source": [
    "Let us now use this function to continue our previous conversation, and find out the highest paying jobs out of the ones recommended in the first response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f299e-6cc6-4a75-bb9c-44bd309fb2e8",
   "metadata": {},
   "source": [
    "We will use the same system message (system_msg) used in previous call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef1646-2c25-47cd-9952-001d0034f98d",
   "metadata": {},
   "source": [
    "We create user_assistant_messages list as follows - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7da14a5e-ef43-4e5e-9ff5-9aeee2657620",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_msg1 = user_msg\n",
    "model_response1 = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "user_msg2 = 'based on the suggestions, choose the 3 jobs with highest average salary'\n",
    "user_assistant_msgs = [user_msg1, model_response1, user_msg2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5818d1-85f5-4d59-a6e6-84fa5f92a964",
   "metadata": {},
   "source": [
    "Note that we used the original prompt as the first user message (user_msg1), the response to that prompt as the first model response message (model_response1), and our new prompt as the second user message (user_msg2).\n",
    "\n",
    "Finally, we add them to the user_assistant_messages list in order of their occurrence in the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ff2a6-b81c-4213-a9a2-2bb68d6ec16f",
   "metadata": {},
   "source": [
    "We now call the continue_chat() function to get the next response in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4946e85-abe9-43e3-8ce6-75be57a91cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = continue_chat(system_msg, user_assistant_msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18388927-b53e-4b69-8f53-b8d120da1851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the suggestions and assuming the resume has the necessary qualifications and experience, here are three jobs with high average salaries:\n",
      "\n",
      "1. Financial Analyst: Financial analysts typically have a higher average salary due to their expertise in analyzing financial data and providing insights and recommendations for businesses. The average salary for financial analysts can range from $60,000 to $100,000 or higher, depending on experience and location.\n",
      "\n",
      "2. Auditor: Auditors play a crucial role in ensuring the accuracy and compliance of financial statements and internal controls. They are in high demand, especially in large accounting firms. The average salary for auditors can range from $50,000 to $90,000 or higher, depending on experience and location.\n",
      "\n",
      "3. Tax Accountant: Tax accountants specialize in tax planning, compliance, and preparing tax returns for individuals and businesses. Given the complexity of tax laws, tax accountants are in high demand, particularly during tax season. The average salary for tax accountants can range from $55,000 to $85,000 or higher, depending on experience and location.\n",
      "\n",
      "Please note that salaries can vary depending on several factors such as experience, location, industry, and company size. It's always recommended to research specific job markets and conduct salary negotiations based on industry standards and personal qualifications.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0aa66a-6707-4c33-9995-58b63d06340c",
   "metadata": {},
   "source": [
    "## Chat with Large PDFs using ChatGPT API and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb9b7ce-05a2-458c-b9b6-5b98b07d4173",
   "metadata": {},
   "source": [
    "The code tutorial shown above fails for very large PDFs. Let us illustrate this with an example. We will try to chat with BCG's \"2022 Annual Sustainability Report\", a large PDF published by the Boston Consulting Group (BCG) on their general impact in the industry. We execute the code shown below -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc1dfb47-ef56-4a75-a2a4-1423a00d1583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251848\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "pdf_file_obj = open('bcg-2022-annual-sustainability-report-apr-2023.pdf', 'rb')\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file_obj)\n",
    "num_pages = len(pdf_reader.pages)\n",
    "detected_text = ''\n",
    "\n",
    "for page_num in range(num_pages):\n",
    "    page_obj = pdf_reader.pages[page_num]\n",
    "    detected_text += page_obj.extract_text() + '\\n\\n'\n",
    "\n",
    "pdf_file_obj.close()\n",
    "print(len(detected_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d084d-04f5-4880-85f9-cb3ae664a7c0",
   "metadata": {},
   "source": [
    "We can see that the PDF is super large, and the length of the detected_text string variable is roughly 250k.\n",
    "\n",
    "Let us now try chatting with the PDF -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "972c4d72-ab03-4b06-afdb-5a3b37aa5985",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens. However, your messages resulted in 54957 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124msummarize this PDF in 500 words.\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m      7\u001b[0m user_msg \u001b[38;5;241m=\u001b[39m detected_text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m query\n\u001b[0;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_msg\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_msg\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 54957 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "system_msg = ''\n",
    "\n",
    "query = '''\n",
    "summarize this PDF in 500 words.\n",
    "'''\n",
    "\n",
    "user_msg = detected_text + '\\n\\n' + query\n",
    "\n",
    "response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                        messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                                         {\"role\": \"user\", \"content\": user_msg}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77b64e1-ad09-4aa3-a633-9e2a9ae9bd18",
   "metadata": {},
   "source": [
    "We get an error message saying that we have hit the prompt length threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364eebdc-44b9-4d49-a0fb-ec0216620db6",
   "metadata": {},
   "source": [
    "This happens because for large PDFs with lots of text, the request payload we send to OpenAI becomes too large, and OpenAI returns an error saying that we have hit the prompt length threshold.\n",
    "\n",
    "Let us now learn how to remove this bottleneck.\n",
    "\n",
    "Enter LangChain. LangChain is an innovative technology that functions as a bridge -  linking large language models (LLMs) with practical applications like Python programming, PDFs, CSV files, or databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133610e2-7f07-4c12-9f97-f4ddabd6954e",
   "metadata": {},
   "source": [
    "Let us import the required dependencies and get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c49eb2b-3708-4926-b0bd-4b38d901a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c464f6f9-1c02-448f-a8e9-374e896cb8e7",
   "metadata": {},
   "source": [
    "We load the PDF using PyPDF loader for LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f133dabb-cffa-4488-bfa2-2eaefe6eb513",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"bcg-2022-annual-sustainability-report-apr-2023.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c1956-221f-47ab-9628-cf1911da53fd",
   "metadata": {},
   "source": [
    "We will perform chunking and split the text using LangChain text splitters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd94780d-8814-4991-a245-cee2019e2261",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.create_documents([detected_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f5ad8-22a1-4892-8061-6a49879ae312",
   "metadata": {},
   "source": [
    "We create a vector database using the chunks. We will save it the database for future use as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "591f777e-a3c8-4f92-ba74-87664c62748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'index_store'\n",
    "vector_index = FAISS.from_documents(texts, OpenAIEmbeddings())\n",
    "vector_index.save_local(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4798c771-1a80-4dc7-8554-dec20b73c628",
   "metadata": {
    "tags": []
   },
   "source": [
    "We now load the database. Using the database, we configure a retriever and then create a chat object. This chat object (qa_interface) will be used to chat with the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8625c8b-8cb4-4893-8c2b-0a7686ad359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = FAISS.load_local('index_store', OpenAIEmbeddings())\n",
    "retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":6})\n",
    "qa_interface = RetrievalQA.from_chain_type(llm=ChatOpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba03911-8774-428a-818d-471290ae5379",
   "metadata": {},
   "source": [
    "We can now start chatting with the PDF. Let us ask the PDF to list measures taken to address diseases occurring in developing industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7ec8058-5ca5-463b-bca0-d82655fe927e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = qa_interface(\"List measures taken to address diseases occuring in developing industries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef604219-23af-4a27-8e43-9ec31800b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_interface(\"What is BCG leadership?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ba31977-c58a-4ce9-9e02-ca10441abfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCG leadership refers to the individuals who hold managing director, partner, and other senior leadership roles within Boston Consulting Group. These leaders are responsible for setting the firm's strategy, vision, and direction, as well as making decisions regarding investments, policy, growth, business mix, people, and philosophy. They work closely with clients and teams to tackle challenges and capture opportunities, with the goal of empowering organizations to grow, build sustainable competitive advantage, and drive positive societal impact.\n"
     ]
    }
   ],
   "source": [
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98144ad4-c83b-457d-a218-d83fcf2d87fd",
   "metadata": {},
   "source": [
    "So far, we've used the RetrievalQA chain, a LangChain type for pulling document pieces from a vector store and asking one question about them. But, sometimes we need to have a full conversation about a document, including referring to topics we've already talked about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d89fa5-8647-4c60-9900-ce80e63e5edf",
   "metadata": {},
   "source": [
    "Thankfully, LangChain has us covered. To make this possible, our system needs a memory or conversation history.  Instead of the RetrievalQA chain, we'll use the ConversationalRetrievalChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "333f16b7-d173-4e71-9754-c7afab10afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_interface = ConversationalRetrievalChain.from_llm(ChatOpenAI(temperature=0), retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2949d78f-9e99-457f-a78b-8b4ad86d926f",
   "metadata": {},
   "source": [
    "Let's ask the PDF to reveal the context in which Morocco is mentioned in the report.\n",
    "\n",
    "'chat_history' parameter is a list contains past conversation history. For the first message, this list will be empty.\n",
    "\n",
    "'question' parameter is used to send our message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "beea0fa8-ef31-4e8d-be1f-7606d17db122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morocco is mentioned in the report in the context of social protection reforms and the expansion of universal health care coverage. The report highlights the government's efforts to improve health access and strengthen social protection programs in Morocco, with a focus on providing support to vulnerable populations. It mentions that as of December 1, 2022, more than 90% of Morocco's population has access to universal health care, which has led to significant improvements in health access for millions of vulnerable families.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"in what context is Morocco mentioned in the report?\"\n",
    "result = conv_interface({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e09a97-ceee-4320-a5f3-15dbce7bb1ed",
   "metadata": {},
   "source": [
    "Let us now continue the conversation by updating the chat_history variable and ask the PDF to give some statistics around this. We append the messages in order of appearance in the conversation. We first append our initial message followed by the first response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4cef65d2-2a17-4b22-811e-d1e3cc386654",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append((query, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c854acef-2c8c-4518-91d8-97b16d11d0fa",
   "metadata": {},
   "source": [
    "We now add our new question along with the updated chat_history to continue the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2f84982-b5f7-4f38-8ff2-9e73663faa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the report, as of December 1, 2022, more than 90% of Morocco's population now has access to universal health care, up from 42% just months before. This rapid increase in access to health care has enabled millions of vulnerable families to benefit from significant health access improvements. Additionally, around 10 million of Morocco's most vulnerable citizens have been integrated into the universal health care scheme. The report also mentions that the social protection reform in Morocco has already yielded positive outcomes, with more to come.\n"
     ]
    }
   ],
   "source": [
    "query = \"give some statistics around this.\"\n",
    "result = conv_interface({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9955e9-2404-456c-a77d-ebd7073eefe4",
   "metadata": {},
   "source": [
    "The result uses the context gained by knowing the conversation history, and provides another great response! We can keep updating the chat_history variable and further continue our conversation using this method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ddf19-3634-4554-b9a0-22fcaa15eef8",
   "metadata": {},
   "source": [
    "## Build PDF Automations using OpenAI GPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f422d523-b6ca-4cab-b815-9003eb8fe392",
   "metadata": {},
   "source": [
    "Let us now explore automations involving PDF tasks that can be implemented using GPT API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b610220-b182-4edb-8cf4-ed2c640daeb6",
   "metadata": {},
   "source": [
    "#### Automation 3 - Recipe Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ecc623-9789-4a51-9454-d3a1beb98395",
   "metadata": {},
   "source": [
    "We can even feed our favorite cookbooks to GPT API, and ask it to give recipe recommendations based on our inputs. Let us look at an example. We use the Brakes' Meals n More recipe cookbook, and talk to it using LangChain. Let us ask it to give recommendations based on the ingredients we have at home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d3d44353-9b08-4d09-82c0-dde36470bd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context, the information provided does not directly answer the user's question about what to cook with minced meat and kidney beans. Therefore, I don't have enough information to provide a specific answer. However, a popular dish that includes minced meat and kidney beans is chili con carne. You can cook the minced meat with onions, garlic, and spices, then add kidney beans, tomatoes, and broth. Simmer it for a while and serve it with rice or tortilla chips.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "import os\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"] = '',
    "directory = 'index_store'\n",
    "\n",
    "loader = PyPDFLoader(\"meals-more-recipes.pdf\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.create_documents([detected_text])\n",
    "\n",
    "directory = 'index_store'\n",
    "vector_index = FAISS.from_documents(texts, OpenAIEmbeddings())\n",
    "vector_index.save_local(directory)\n",
    "\n",
    "vector_index = FAISS.load_local('index_store', OpenAIEmbeddings())\n",
    "retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":6})\n",
    "qa_interface = RetrievalQA.from_chain_type(llm=ChatOpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "\n",
    "response = qa_interface(\"\"\"\n",
    "I have a lot of broccoli and tomatoes at home. \n",
    "Recommend recipe for some meal I can make at home using these.\n",
    "\"\"\")\n",
    "\n",
    "response = qa_interface(\"\"\"\n",
    "I have got minced meat and kidney beans in my fridge. What should I cook with it?\n",
    "\"\"\")\n",
    "\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e188d-e9f1-40b5-950f-f9aeb4d7121f",
   "metadata": {},
   "source": [
    "Upon execution, the PDF recommends a recipe for a meal that can be prepared using the mentioned ingredients!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f154b91-9bde-463b-a584-ebe146902efc",
   "metadata": {},
   "source": [
    "#### Automation 4 - Automated Test Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85030b2-0966-486d-854a-95d2bdc9d771",
   "metadata": {},
   "source": [
    "You can feed textbooks and automate creation of complete question papers and tests using GPT API. The LLM can even generate the marking scheme for you!\n",
    "We use the textbook Advanced High-School Mathematics by David B. Surowski and ask the LLM to create a question paper with a marking scheme for a particular chapter in the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35da741-fc85-40b1-ae29-8d70a6e47738",
   "metadata": {},
   "source": [
    "We execute the below code - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef0d9202-2058-4fb5-a859-5bf213673789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is the 9-point circle in Euclidean geometry and what are its properties? (4 marks)\n",
      "\n",
      "2. Explain the construction of the 9-point circle and how it is related to the triangle's properties. (5 marks)\n",
      "\n",
      "3. Prove that the 9-point circle passes through the midpoints of the triangle's sides, the feet of its altitudes, and the midpoints of the line segments joining the vertices to the orthocenter. (6 marks)\n",
      "\n",
      "4. Discuss the significance of the 9-point circle in triangle geometry and its applications in solving geometric problems. (3 marks)\n",
      "\n",
      "5. Compare and contrast the 9-point circle with other important circles in Euclidean geometry, such as the circumcircle and incircle. (2 marks)\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '',
    "directory = 'index_store'\n",
    "\n",
    "loader = PyPDFLoader(\"further.pdf\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.create_documents([detected_text])\n",
    "\n",
    "directory = 'index_store'\n",
    "vector_index = FAISS.from_documents(texts, OpenAIEmbeddings())\n",
    "vector_index.save_local(directory)\n",
    "\n",
    "vector_index = FAISS.load_local('index_store', OpenAIEmbeddings())\n",
    "retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":6})\n",
    "qa_interface = RetrievalQA.from_chain_type(llm=ChatOpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "\n",
    "response = qa_interface(\"\"\"\n",
    "#list 5 questions of 20 marks total of varying difficuly and weightage based on the topic \"Euclidian Geometry\"\n",
    "eli5 9-point circle\n",
    "\"\"\")\n",
    "\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f36101-9ae6-4854-8912-341873902998",
   "metadata": {},
   "source": [
    "The LLM reads the PDF textbook and create the question paper for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae3d4a-2021-4c24-b0ab-01e389381a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
